{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreenhousePage:\n",
    "    def __init__(self, slug):\n",
    "        self.slug = slug\n",
    "        self.run_date = datetime.date.today().strftime(\"%Y-%b-%d\")  \n",
    "        self.url = 'https://boards.greenhouse.io/' + self.slug\n",
    "        self.load_soup()\n",
    "        self.title = self.soup.find(\"meta\", property = \"og:title\")['content']\n",
    "        self.sections = []\n",
    "        self.get_sections()\n",
    "        self.jobs_data = [['category','job_title','page_link','job_id', 'location', 'run_date']]\n",
    "        self.load_jobs()\n",
    "        self.load_dataframe()\n",
    "    \n",
    "    def load_jobs(self):\n",
    "        sec_divs = self.soup.findAll(\"section\")\n",
    "        for sec in sec_divs:\n",
    "            \n",
    "            all_links = sec.findAll('a', attrs={'data-mapped': 'true'})\n",
    "            for link in all_links:\n",
    "                category = link.find_previous(\"h3\").text\n",
    "                job_title = link.contents[0]\n",
    "                page_link = 'https://boards.greenhouse.io' + link['href']\n",
    "                job_id = link['href'][len(self.slug + \"/jobs/\") + 1:]\n",
    "                location = link.find_next_sibling('span').text\n",
    "                self.jobs_data.append([category, job_title, page_link, job_id, location, self.run_date])\n",
    "    \n",
    "    def load_dataframe(self):\n",
    "        self.jobs = pd.DataFrame(self.jobs_data[1:], columns = self.jobs_data[0])\n",
    "            \n",
    "    def get_sections(self):\n",
    "        if not self.soup:\n",
    "            self.load_soup()\n",
    "        sec_divs = self.soup.findAll(\"section\")\n",
    "        for sec in sec_divs:\n",
    "            try:\n",
    "                sec_title = sec.find(\"h3\").text\n",
    "                self.sections.append(sec_title)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    def load_soup(self):\n",
    "        response = requests.get(self.url)\n",
    "        self.soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "    def print_html(self):\n",
    "        print(self.soup.prettify())\n",
    "    \n",
    "    def to_csv(self):\n",
    "        self.jobs.to_csv(self.run_date + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base job page URL for Bernie 2020\n",
    "#base_url = 'https://boards.greenhouse.io/bernie2020'\n",
    "#base_url = 'https://boards.greenhouse.io/gitlab'\n",
    "\n",
    "gitlab_page = GreenhousePage('gitlab')\n",
    "bernie_page = GreenhousePage('bernie2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernie_page.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are currently 192 open jobs\n"
     ]
    }
   ],
   "source": [
    "# Get all job links\n",
    "# <div class = \"opening\"> ... <a href...\"\"\n",
    "soup = gitlab_page.soup\n",
    "open_jobs = []\n",
    "all_links = soup.findAll('a', attrs={'data-mapped': 'true'})\n",
    "for link in all_links:\n",
    "    category = '' #@TODO Find previous h3 tag\n",
    "    job_title = link.contents[0]\n",
    "    page_link = 'https://boards.greenhouse.io' + link['href']\n",
    "    open_jobs.append({'job_title': job_title, 'link': page_link})\n",
    "print(f\"There are currently {len(open_jobs)} open jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the word \"sql\" 3 times in Operations Analyst, Development (https://boards.greenhouse.io/gitlab/jobs/4371373002)\n",
      "Found the word \"sql\" 3 times in Backend Engineer, Database (https://boards.greenhouse.io/gitlab/jobs/4473989002)\n",
      "Found the word \"sql\" 2 times in Backend Engineer, Search (https://boards.greenhouse.io/gitlab/jobs/4392358002)\n",
      "Found the word \"sql\" 2 times in Senior Security Engineer, Abuse (https://boards.greenhouse.io/gitlab/jobs/4489255002)\n",
      "Found the word \"sql\" 2 times in Finance Business Partner, Sales (https://boards.greenhouse.io/gitlab/jobs/4500567002)\n",
      "Found the word \"sql\" 2 times in Business Systems Analyst (https://boards.greenhouse.io/gitlab/jobs/4450334002)\n",
      "Found the word \"sql\" 5 times in Data Analyst (https://boards.greenhouse.io/gitlab/jobs/4416681002)\n",
      "Found the word \"sql\" 5 times in Data Analyst (https://boards.greenhouse.io/gitlab/jobs/4441205002)\n",
      "Found the word \"sql\" 5 times in Data Engineer (Product/Engineering) (https://boards.greenhouse.io/gitlab/jobs/4513755002)\n",
      "Found the word \"sql\" 4 times in Data Engineer (S&M) (https://boards.greenhouse.io/gitlab/jobs/4456846002)\n",
      "Found the word \"sql\" 2 times in Director of Data and Analytics (https://boards.greenhouse.io/gitlab/jobs/4325127002)\n",
      "{'https://boards.greenhouse.io/gitlab/jobs/4371373002': {'job_title': 'Operations Analyst, Development', 'result_count': 3}, 'https://boards.greenhouse.io/gitlab/jobs/4473989002': {'job_title': 'Backend Engineer, Database', 'result_count': 3}, 'https://boards.greenhouse.io/gitlab/jobs/4392358002': {'job_title': 'Backend Engineer, Search', 'result_count': 2}, 'https://boards.greenhouse.io/gitlab/jobs/4489255002': {'job_title': 'Senior Security Engineer, Abuse', 'result_count': 2}, 'https://boards.greenhouse.io/gitlab/jobs/4500567002': {'job_title': 'Finance Business Partner, Sales', 'result_count': 2}, 'https://boards.greenhouse.io/gitlab/jobs/4450334002': {'job_title': 'Business Systems Analyst', 'result_count': 2}, 'https://boards.greenhouse.io/gitlab/jobs/4416681002': {'job_title': 'Data Analyst', 'result_count': 5}, 'https://boards.greenhouse.io/gitlab/jobs/4441205002': {'job_title': 'Data Analyst', 'result_count': 5}, 'https://boards.greenhouse.io/gitlab/jobs/4513755002': {'job_title': 'Data Engineer (Product/Engineering)', 'result_count': 5}, 'https://boards.greenhouse.io/gitlab/jobs/4456846002': {'job_title': 'Data Engineer (S&M)', 'result_count': 4}, 'https://boards.greenhouse.io/gitlab/jobs/4325127002': {'job_title': 'Director of Data and Analytics', 'result_count': 2}}\n"
     ]
    }
   ],
   "source": [
    "# Process open jobs and save text\n",
    "keyword = 'sql'\n",
    "final_results = {}\n",
    "for job in open_jobs:\n",
    "    job_title = job['job_title']\n",
    "    link = job['link']\n",
    "   \n",
    "    search_response = requests.get(link)\n",
    "    search_soup = BeautifulSoup(search_response.text, 'html.parser')\n",
    "    results = search_soup.body.find_all(string = re.compile('.*{0}.*'.format(keyword), re.IGNORECASE), recursive = True)\n",
    "    \n",
    "    if len(results) > 0:\n",
    "        final_results[link] = {'job_title': job_title, 'result_count': len(results)}\n",
    "        print(f'Found the word \"{keyword}\" {len(results)} times in {job_title} ({link})')\n",
    "                \n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
