{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreenhousePage:\n",
    "    def __init__(self, slug):\n",
    "        self.slug = slug\n",
    "        self.run_date = datetime.date.today().strftime(\"%Y-%b-%d\")  \n",
    "        self.url = 'https://boards.greenhouse.io/' + self.slug\n",
    "        self.load_soup()\n",
    "        self.title = self.soup.find(\"meta\", property = \"og:title\")['content']\n",
    "        self.sections = []\n",
    "        self.get_sections()\n",
    "        self.jobs_data = [['category','job_title','page_link','job_id', 'location', 'run_date']]\n",
    "        self.load_jobs()\n",
    "        self.load_dataframe()\n",
    "    \n",
    "    def load_jobs(self):\n",
    "        sec_divs = self.soup.findAll(\"section\")\n",
    "        for sec in sec_divs:\n",
    "            \n",
    "            all_links = sec.findAll('a', attrs={'data-mapped': 'true'})\n",
    "            for link in all_links:\n",
    "                category = link.find_previous(\"h3\").text\n",
    "                job_title = link.contents[0]\n",
    "                page_link = 'https://boards.greenhouse.io' + link['href']\n",
    "                job_id = link['href'][len(self.slug + \"/jobs/\") + 1:]\n",
    "                location = link.find_next_sibling('span').text\n",
    "                self.jobs_data.append([category, job_title, page_link, job_id, location, self.run_date])\n",
    "    \n",
    "    def load_dataframe(self):\n",
    "        self.jobs = pd.DataFrame(self.jobs_data[1:], columns = self.jobs_data[0])\n",
    "            \n",
    "    def get_sections(self):\n",
    "        if not self.soup:\n",
    "            self.load_soup()\n",
    "        sec_divs = self.soup.findAll(\"section\")\n",
    "        for sec in sec_divs:\n",
    "            try:\n",
    "                sec_title = sec.find(\"h3\").text\n",
    "                self.sections.append(sec_title)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    def load_soup(self):\n",
    "        response = requests.get(self.url)\n",
    "        self.soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "    def print_html(self):\n",
    "        print(self.soup.prettify())\n",
    "    \n",
    "    def to_csv(self):\n",
    "        self.jobs.to_csv(self.run_date + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base job page URL for Bernie 2020\n",
    "#base_url = 'https://boards.greenhouse.io/bernie2020'\n",
    "#base_url = 'https://boards.greenhouse.io/gitlab'\n",
    "\n",
    "gitlab_page = GreenhousePage('gitlab')\n",
    "bernie_page = GreenhousePage('bernie2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernie_page.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are currently 75 open jobs\n"
     ]
    }
   ],
   "source": [
    "# Get all job links\n",
    "# <div class = \"opening\"> ... <a href...\"\"\n",
    "soup = bernie_page.soup\n",
    "open_jobs = []\n",
    "all_links = soup.findAll('a', attrs={'data-mapped': 'true'})\n",
    "for link in all_links:\n",
    "    category = '' #@TODO Find previous h3 tag\n",
    "    job_title = link.contents[0]\n",
    "    page_link = 'https://boards.greenhouse.io' + link['href']\n",
    "    open_jobs.append({'job_title': job_title, 'link': page_link})\n",
    "print(f\"There are currently {len(open_jobs)} open jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the word \"sql\" 3 times in Data Analyst (https://boards.greenhouse.io/bernie2020/jobs/4269064002)\n",
      "Found the word \"sql\" 2 times in Full Stack Engineer (https://boards.greenhouse.io/bernie2020/jobs/4244285002)\n",
      "Found the word \"sql\" 4 times in Iowa Data Intern (https://boards.greenhouse.io/bernie2020/jobs/4374451002)\n",
      "Found the word \"sql\" 5 times in National Data Desk (https://boards.greenhouse.io/bernie2020/jobs/4489865002)\n"
     ]
    }
   ],
   "source": [
    "# Process open jobs and save text\n",
    "keyword = 'sql'\n",
    "final_results = {}\n",
    "for job in open_jobs:\n",
    "    job_title = job['job_title']\n",
    "    link = job['link']\n",
    "   \n",
    "    search_response = requests.get(link)\n",
    "    search_soup = BeautifulSoup(search_response.text, 'html.parser')\n",
    "    results = search_soup.body.find_all(string = re.compile('.*{0}.*'.format(keyword), re.IGNORECASE), recursive = True)\n",
    "    \n",
    "    if len(results) > 0:\n",
    "        final_results[link] = {'job_title': job_title, 'result_count': len(results)}\n",
    "        print(f'Found the word \"{keyword}\" {len(results)} times in {job_title} ({link})')\n",
    "                \n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
